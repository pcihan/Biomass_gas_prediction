# -*- coding: utf-8 -*-
"""
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12oJG00jNjr3DWpC91ZY0YJSVChfur1wj
"""

# Finding outliers in the original dataset
import numpy as np
import pandas as pd
from scipy.stats import zscore

# Loading the data
data = pd.read_excel("Data1-original.xlsx")

# Calculating the total number of samples in the dataset
sample_size = data.shape[0]

# Calculating the Z-scores
z_scores = np.abs(zscore(data))

# Identifying Z-scores greater than 3
outliers = (z_scores > 3)  # Boolean array; True indicates outliers

# Counting the number of Z-scores greater than 3 for each column
outliers_count = outliers.sum(axis=0)  # Number of outliers per column
print("Outliers Count:", outliers_count)

# Calculating the total number of outliers (unique rows)
unique_outlier_rows = np.unique(np.where(outliers)[0])  # Unique row indices
total_outliers = len(unique_outlier_rows)

# Calculating the percentage of outliers
outlier_percentage = (total_outliers / sample_size) * 100

# Printing the percentage of outliers and whether they can be removed
print(f"\nTotal Number of Samples: {sample_size}")
print(f"Total Number of Outliers: {total_outliers}")
print(f"Outlier Percentage: %{outlier_percentage:.2f}")

# A condition to remove outliers from the dataset
threshold = 10  # The outlier percentage must be less than 10%
if outlier_percentage < threshold:
    print(f"Since the outlier percentage is %{outlier_percentage:.2f}, outliers can be removed from the dataset.")
else:
    print(f"Since the outlier percentage is %{outlier_percentage:.2f}, outliers should not be removed from the dataset.")

# Detecting outliers in the original dataset
import numpy as np
import pandas as pd
from scipy.stats import zscore

data = pd.read_excel("Data1-original.xlsx")

z_scores = np.abs(zscore(data))

# Identifying Z-scores greater than 3
outliers = (z_scores > 3)  # Boolean array; True indicates outliers

# Finding the rows and columns where outliers are located
outlier_indices = np.where(outliers)[0]  # Indices of rows with outliers
outlier_columns = np.where(outliers)[1]  # Indices of columns with outliers

# Printing the rows and columns with outliers
outlier_data = data.iloc[outlier_indices]  # Rows with outliers
outlier_columns_names = data.columns[outlier_columns]  # Names of columns with outliers

# Printing the outlier data
print("Rows with Outliers:")
print(outlier_data)

# Removing outliers from the dataset
data_cleaned = data.drop(outlier_indices)  # Removing outliers

data_cleaned.to_excel('data_cleaned.xlsx', index=False)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from scipy.stats import shapiro

data = pd.read_excel("Data1.xlsx")
X = data.iloc[:, :-1]  # inputs

# Normalization
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)

X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)

plt.figure(figsize=(14, 6))

# Violin Plot for Original data
plt.subplot(1, 2, 1)
sns.violinplot(data=X)
plt.title('Violin Plot of Original Input Features')
plt.ylabel('Values')

# Violin Plot for Normalized data
plt.subplot(1, 2, 2)
sns.violinplot(data=X_normalized_df)
plt.title('Violin Plot of Normalized Input Features')
plt.ylabel('Values')

plt.tight_layout()
plt.show()

# Shapiro-Wilk test
test_results = {}
for column in X.columns:
    stat, p_value = shapiro(X[column])
    test_results[column] = {
        'Statistic': stat,
        'p-value': p_value,
        'Normal Distribution': p_value > 0.05
    }

results_df = pd.DataFrame(test_results).T
results_df.index.name = 'Feature'
results_df.columns = ['Statistic', 'p-value', 'Normal Distribution']

print("\nShapiro-Wilk Test Results:")
print(results_df)

results_df.to_excel('Shapiro_Wilk_Test_Results.xlsx')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

f_max = X.max(axis=0)  # The maximum values of each input variable
f_min = X.min(axis=0)  # The minimum values of each input variable

N = f_max - f_min
N_total = N.sum()
S_percentage = N / N_total

# Normalization (StandardScaler)
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)

X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)

f_max_normalized = X_normalized_df.max(axis=0)
f_min_normalized = X_normalized_df.min(axis=0)


N_normalized = f_max_normalized - f_min_normalized
N_total_normalized = N_normalized.sum()
S_percentage_normalized = N_normalized / N_total_normalized

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.set(style="whitegrid")
sorted_idx = S_percentage.argsort()
cmap = sns.color_palette("flare", as_cmap=True)
sns.barplot(x=S_percentage[sorted_idx], y=X.columns[sorted_idx], palette=cmap(S_percentage[sorted_idx]))
plt.axvline(x=0.05, color="red", linestyle="--", label="Sensitivity Threshold (0.05)")
plt.xlabel("Influence of input parameters", fontsize=11)
plt.ylabel("Input Variable", fontsize=11)
plt.title("Sensitivity Analyses for Original Dataset", fontsize=13)
plt.legend()

plt.subplot(1, 2, 2)
sns.set(style="whitegrid")
sorted_idx_normalized = S_percentage_normalized.argsort()
sns.barplot(x=S_percentage_normalized[sorted_idx_normalized], y=X.columns[sorted_idx_normalized], palette=cmap(S_percentage_normalized[sorted_idx_normalized]))
plt.axvline(x=0.05, color="red", linestyle="--", label="Sensitivity Threshold (0.05)")
plt.xlabel("Influence of input parameters", fontsize=11)
plt.ylabel("Input Variable", fontsize=11)
plt.title("Sensitivity Analyses for Normalized Dataset", fontsize=13)
plt.legend()

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import mutual_info_regression

# Loading the data
data = pd.read_excel("Data1.xlsx")

# Separating independent and dependent variables
#X = data.drop('CO', axis=1)
#y = data['CO']
#X = data.drop('CO2', axis=1)
#y = data['CO2']
#X = data.drop('H2', axis=1)
#y = data['H2']
X = data.drop('CH4', axis=1)  # Dropping CH4 from features (independent variables)
y = data['CH4']  # CH4 as the target variable (dependent variable)

# Correlation Matrix
corr_matrix = data.corr()
mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)  # Mask for the upper triangle

fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Heatmap creation
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="viridis", linewidths=0.5, square=True, cbar=False, ax=axes[0], mask=mask, annot_kws={"size": 10})
#axes[0].set_title("Linear Correlation - CO output")
#axes[0].set_title("Linear Correlation - CO2 output")
#axes[0].set_title("Linear Correlation - H2 output")
axes[0].set_title("Linear Correlation - CH4 output")  # Title for CH4 output correlation heatmap

# Mutual Information (Non-Linear)
mutual_info = mutual_info_regression(X, y)  # Calculate mutual information between features and target
mutual_info_df = pd.DataFrame(mutual_info, index=X.columns, columns=['Mutual Information'])  # Create DataFrame for mutual info

# Printing mutual information values
print("Mutual Information")
print(mutual_info_df)

# Bar chart creation
ax = mutual_info_df.plot(kind='bar', ax=axes[1])  # Plot bar chart of mutual information values
#axes[1].set_title("Non-Linear Correlation - CO output")
#axes[1].set_title("Non-Linear Correlation - CO2 output")
#axes[1].set_title("Non-Linear Correlation - H2 output")
axes[1].set_title("Non-Linear Correlation - CH4 output")  # Title for CH4 non-linear correlation bar chart

# Annotating bars with the values
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9)

# Rotating and aligning x-axis labels
axes[1].tick_params(axis='x', labelrotation=45)
axes[1].set_xticklabels(axes[1].get_xticklabels(), ha='right')

plt.tight_layout()
plt.show()

pip install bayesian-optimization

# RF model for Optimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
from bayes_opt import BayesianOptimization

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the highest R value)
def rf_evaluate(n_estimators, max_depth, min_samples_split):
    model = RandomForestRegressor(
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        min_samples_split=int(min_samples_split),
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return corr_coef

# Parameter ranges
pbounds = {
    'n_estimators': (10, 500),
    'max_depth': (2, 100),
    'min_samples_split': (2, 50)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=rf_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the highest correlation value
best_params = optimizer.max['params']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])
best_min_samples_split = int(best_params['min_samples_split'])

# Create model for optimum condition
best_model = RandomForestRegressor(
    n_estimators=best_n_estimators,
    max_depth=best_max_depth,
    min_samples_split=best_min_samples_split,
    random_state=42
)

# Model training
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: n_estimators={best_n_estimators}, max_depth={best_max_depth}, min_samples_split={best_min_samples_split}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")
print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test,
    'Predicred': y_pred
})
results_df.to_excel('RF-Optimum-actual-predicted-values.xlsx', index=False)

# RF model for SubOptimum
import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the lowest R value)
def rf_evaluate(n_estimators, max_depth, min_samples_split):
    model = RandomForestRegressor(
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        min_samples_split=int(min_samples_split),
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return -corr_coef

# Parameter ranges
pbounds = {
    'n_estimators': (10, 500),
    'max_depth': (2, 100),
    'min_samples_split': (2, 50)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=rf_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the lowest correlation value.
best_params = optimizer.max['params']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])
best_min_samples_split = int(best_params['min_samples_split'])

# Create model for suboptimum condition
worst_model = RandomForestRegressor(
    n_estimators=best_n_estimators,
    max_depth=best_max_depth,
    min_samples_split=best_min_samples_split,
    random_state=42
)

# Model training
worst_model.fit(X_train, y_train)
y_pred = worst_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for Suboptimum condition: n_estimators={best_n_estimators}, max_depth={best_max_depth}, min_samples_split={best_min_samples_split}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")
print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred
})

results_df.to_excel('RF-SubOptimum-actual-predicted-values.xlsx', index=False)

#XGBoost model for optimum
import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from xgboost import XGBRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the highest R value)
def xgb_evaluate(learning_rate, n_estimators, max_depth, subsample, min_child_weight):
    model = XGBRegressor(
        learning_rate=learning_rate,
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        subsample=subsample,
        min_child_weight=int(min_child_weight),
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return corr_coef

# Parameter ranges
pbounds = {
    'learning_rate': (0.01, 0.3),
    'n_estimators': (10, 500),
    'max_depth': (2, 10),
    'subsample': (0.5, 1.0),
    'min_child_weight': (1, 10)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=xgb_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the highest correlation value
best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])
best_subsample = best_params['subsample']
best_min_child_weight = int(best_params['min_child_weight'])

# Create model for optimum condition
best_model = XGBRegressor(
    learning_rate=best_learning_rate,
    n_estimators=best_n_estimators,
    max_depth=best_max_depth,
    subsample=best_subsample,
    min_child_weight=best_min_child_weight,
    random_state=42
)
# Model training
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: n_estimators={best_n_estimators}, max_depth={best_max_depth}, learning_rate={best_learning_rate},subsample={best_subsample}, min_samples_split={best_min_samples_split}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")

print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test,
    'Predicred': y_pred
})
results_df.to_excel('XGBoost-Optimum-actual-predicted-values.xlsx', index=False)

#XGBoost model for SubOptimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from xgboost import XGBRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the lowest R value)
def xgb_evaluate(n_estimators, max_depth, learning_rate, min_child_weight, subsample):
    model = XGBRegressor(
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        learning_rate=learning_rate,
        min_child_weight=int(min_child_weight),
        subsample=subsample,
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return -corr_coef

# Parameter ranges
pbounds = {
    'n_estimators': (10, 500),
    'max_depth': (2, 100),
    'learning_rate': (0.01, 0.3),
    'subsample': (0.5, 1.0),
    'min_child_weight': (1, 10)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=xgb_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)  # Daha fazla başlangıç noktası ve iterasyon

# Obtain the parameters that give the lowest correlation value
best_params = optimizer.max['params']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])
best_learning_rate = best_params['learning_rate']
best_min_child_weight = int(best_params['min_child_weight'])
best_subsample = best_params['subsample']

# Create model for suboptimum condition
worst_model = XGBRegressor(
    n_estimators=best_n_estimators,
    max_depth=best_max_depth,
    learning_rate=best_learning_rate,
    min_child_weight=best_min_child_weight,
    subsample=best_subsample,
    random_state=42,
    verbosity=0
)

# Model training
worst_model.fit(X_train, y_train)
y_pred = worst_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: n_estimators={best_n_estimators}, max_depth={best_max_depth}, learning_rate={best_learning_rate},subsample={best_subsample}, min_samples_split={best_min_samples_split}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")
print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred
})

results_df.to_excel('XGBoost-SubOptimum-actual-predicted-values.xlsx', index=False)

##LightGBM model for Optimum
import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from lightgbm import LGBMRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the highest R value)
def lgb_evaluate(learning_rate, n_estimators, max_depth, num_leaves, subsample):
    model = LGBMRegressor(
        learning_rate=learning_rate,
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        num_leaves=int(num_leaves),
        subsample=subsample,
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return corr_coef

# Parameter ranges
pbounds = {
    'learning_rate': (0.01, 0.3),
    'n_estimators': (10, 500),
    'max_depth': (2, 10),
    'num_leaves': (20, 50),
    'subsample': (0.5, 1.0)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=lgb_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the highest correlation value
best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])
best_num_leaves = int(best_params['num_leaves'])
best_subsample = best_params['subsample']

# Create model for optimum condition
best_model = LGBMRegressor(
    learning_rate=best_learning_rate,
    n_estimators=best_n_estimators,
    max_depth=best_max_depth,
    num_leaves=best_num_leaves,
    subsample=best_subsample,
    random_state=42
)

# Model training
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition:  learning_rate={best_learning_rate},n_estimators={best_n_estimators}, max_depth={best_max_depth},num_leaves={best_num_leaves}, subsample={best_subsample}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")

print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test,
    'Predicred': y_pred
})
results_df.to_excel('LightGBM-Optimum-actual-predicted-values.xlsx', index=False)

#LightGBM model for SubOptimum
import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from lightgbm import LGBMRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the lowest R value)
def lgbm_evaluate(learning_rate, n_estimators, max_depth, num_leaves, subsample):
    model = LGBMRegressor(
        learning_rate=learning_rate,
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        num_leaves=int(num_leaves),
        subsample=subsample,
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return -corr_coef

# Parameter ranges
pbounds = {
    'learning_rate': (0.01, 0.3),
    'n_estimators': (10, 500),
    'max_depth': (2, 10),
    'num_leaves': (20, 50),
    'subsample': (0.5, 1.0)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=lgbm_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the lowest correlation value
best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])
best_num_leaves = int(best_params['num_leaves'])
best_subsample = best_params['subsample']

# Create model for suboptimum condition
worst_model = LGBMRegressor(
    learning_rate=best_learning_rate,
    n_estimators=best_n_estimators,
    max_depth=best_max_depth,
    num_leaves=best_num_leaves,
    subsample=best_subsample,
    random_state=42
)

# Model training
worst_model.fit(X_train, y_train)
y_pred = worst_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition:  learning_rate={best_learning_rate},n_estimators={best_n_estimators}, max_depth={best_max_depth},num_leaves={best_num_leaves}, subsample={best_subsample}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")
print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred
})

results_df.to_excel('LightGBM-SubOptimum-actual-predicted-values.xlsx', index=False)

## EleaticcNET model for Optimum
import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.linear_model import ElasticNet
from bayes_opt import BayesianOptimization

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


def elastic_net_evaluate(alpha, l1_ratio):
    model = ElasticNet(
        alpha=alpha,
        l1_ratio=l1_ratio,
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return corr_coef

# Parameter ranges
pbounds = {
    'alpha': (0.001, 10.0),  # Regularization term
    'l1_ratio': (0.0, 1.0)   # L1 and L2 ratio
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=elastic_net_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the highest correlation value
best_params = optimizer.max['params']
best_alpha = best_params['alpha']
best_l1_ratio = best_params['l1_ratio']

# Create model for optimum condition
best_model = ElasticNet(
    alpha=best_alpha,
    l1_ratio=best_l1_ratio,
    random_state=42
)

# Model training
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: alpha={best_alpha}, l1_ratio={best_l1_ratio}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")

print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test,
    'Predicred': y_pred
})
results_df.to_excel('ElasticNet-Optimum-actual-predicted-values.xlsx', index=False)

# ElasticNET model for SubOptimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.linear_model import ElasticNet
from bayes_opt import BayesianOptimization

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Bayesian Optimization Function (To obtain the lowest R value)
def elastic_net_evaluate(alpha, l1_ratio):
    model = ElasticNet(
        alpha=alpha,
        l1_ratio=l1_ratio,
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return -corr_coef

# Parameter ranges
pbounds = {
    'alpha': (0.001, 10.0),  # Regularization term
    'l1_ratio': (0.0, 1.0)   # L1 and L2 ratio
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=elastic_net_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the lowest correlation value
best_params = optimizer.max['params']
best_alpha = best_params['alpha']
best_l1_ratio = best_params['l1_ratio']

# Create model for suboptimum condition
worst_model = ElasticNet(
    alpha=best_alpha,
    l1_ratio=best_l1_ratio,
    random_state=42
)

# Model training
worst_model.fit(X_train, y_train)
y_pred = worst_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: alpha={best_alpha}, l1_ratio={best_l1_ratio}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")
print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred
})

results_df.to_excel('ElasticNet-SubOptimum-actual-predicted-values.xlsx', index=False)

## ADABoost model for Optimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the highest R value)
def ada_evaluate(learning_rate, n_estimators, max_depth):
    model = AdaBoostRegressor(
        estimator=DecisionTreeRegressor(max_depth=int(max_depth)),
        learning_rate=learning_rate,
        n_estimators=int(n_estimators),
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return corr_coef

# Parameter ranges
pbounds = {
    'learning_rate': (0.01, 1.0),
    'n_estimators': (10, 500),
    'max_depth': (1, 10)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=ada_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the highest correlation value
best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])

# Create model for optimum condition
best_model = AdaBoostRegressor(
    estimator=DecisionTreeRegressor(max_depth=best_max_depth),
    learning_rate=best_learning_rate,
    n_estimators=best_n_estimators,
    random_state=42
)

# Model training
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: learning_rate={best_learning_rate}, n_estimators={best_n_estimators}, max_depth={best_max_depth}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")

print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test,
    'Predicred': y_pred
})
results_df.to_excel('AdaBoost-Optimum-actual-predicted-values.xlsx', index=False)

## ADABoost model for SubOptimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the lowest R value)
def ada_evaluate(learning_rate, n_estimators, max_depth):
    model = AdaBoostRegressor(
        estimator=DecisionTreeRegressor(max_depth=int(max_depth)),
        learning_rate=learning_rate,
        n_estimators=int(n_estimators),
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return -corr_coef

# Parameter ranges
pbounds = {
    'learning_rate': (0.01, 1.0),
    'n_estimators': (10, 500),
    'max_depth': (1, 10)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=ada_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the lowest correlation value
best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])

# Create model for suboptimum condition
worst_model = AdaBoostRegressor(
    estimator=DecisionTreeRegressor(max_depth=best_max_depth),
    learning_rate=best_learning_rate,
    n_estimators=best_n_estimators,
    random_state=42
)

# Model training
worst_model.fit(X_train, y_train)
y_pred = worst_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: learning_rate={best_learning_rate}, n_estimators={best_n_estimators}, max_depth={best_max_depth}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")
print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred
})

results_df.to_excel('ADABoost-SubOptimum-actual-predicted-values.xlsx', index=False)

## Gradient Boosting Regressor model for Optimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from sklearn.ensemble import GradientBoostingRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # outputs

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the highest R value)
def gb_evaluate(learning_rate, n_estimators, max_depth, subsample):
    model = GradientBoostingRegressor(
        learning_rate=learning_rate,
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        subsample=subsample,
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return corr_coef

# Parameter ranges
pbounds = {
    'learning_rate': (0.01, 0.3),
    'n_estimators': (10, 500),
    'max_depth': (2, 10),
    'subsample': (0.5, 1.0)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=gb_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the highest correlation value
best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])
best_subsample = best_params['subsample']

# Create model for optimum condition
best_model = GradientBoostingRegressor(
    learning_rate=best_learning_rate,
    n_estimators=best_n_estimators,
    max_depth=best_max_depth,
    subsample=best_subsample,
    random_state=42
)

# Model training
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: learning_rate={best_learning_rate},n_estimators={best_n_estimators}, max_depth={best_max_depth}, subsample={best_subsample}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")

print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test,
    'Predicred': y_pred
})
results_df.to_excel('GBR-Optimum-actual-predicted-values.xlsx', index=False)

## Gradient Boosting Regressor model for SubOptimum
import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from sklearn.ensemble import GradientBoostingRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the lowest R value)
def gb_evaluate(learning_rate, n_estimators, max_depth, subsample):
    model = GradientBoostingRegressor(
        learning_rate=learning_rate,
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        subsample=subsample,
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return -corr_coef

# Parameter ranges
pbounds = {
    'learning_rate': (0.01, 0.3),
    'n_estimators': (10, 500),
    'max_depth': (2, 10),
    'subsample': (0.5, 1.0)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=gb_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the lowest correlation value
best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_n_estimators = int(best_params['n_estimators'])
best_max_depth = int(best_params['max_depth'])
best_subsample = best_params['subsample']

# Create model for suboptimum condition
worst_model = GradientBoostingRegressor(
    learning_rate=best_learning_rate,
    n_estimators=best_n_estimators,
    max_depth=best_max_depth,
    subsample=best_subsample,
    random_state=42
)

# Model training
worst_model.fit(X_train, y_train)
y_pred = worst_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for Suboptimum condition: learning_rate={best_learning_rate},n_estimators={best_n_estimators}, max_depth={best_max_depth}, subsample={best_subsample}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")
print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred
})

results_df.to_excel('GBR-SubOptimum-actual-predicted-values.xlsx', index=False)

## KNN model for Optimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from sklearn.neighbors import KNeighborsRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the highest R value)
def knn_evaluate(n_neighbors, weights, algorithm):
    model = KNeighborsRegressor(
        n_neighbors=int(n_neighbors),
        weights='uniform' if weights < 0.5 else 'distance',
        algorithm='ball_tree' if algorithm < 0.33 else 'kd_tree' if algorithm < 0.67 else 'brute'
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return corr_coef

# Parameter ranges
pbounds = {
    'n_neighbors': (1, 50),
    'weights': (0, 1),  # 0: uniform, 1: distance
    'algorithm': (0, 1)  # 0: ball_tree, 1: kd_tree, 2: brute
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=knn_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the highest correlation value
best_params = optimizer.max['params']
best_n_neighbors = int(best_params['n_neighbors'])
best_weights = 'uniform' if best_params['weights'] < 0.5 else 'distance'
best_algorithm = 'ball_tree' if best_params['algorithm'] < 0.33 else 'kd_tree' if best_params['algorithm'] < 0.67 else 'brute'

# Create model for optimum condition
best_model = KNeighborsRegressor(
    n_neighbors=best_n_neighbors,
    weights=best_weights,
    algorithm=best_algorithm
)

# Model training
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: n_neighbors={best_n_neighbors}, weights={best_weights}, algorithm={best_algorithm}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")

print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test,
    'Predicred': y_pred
})
results_df.to_excel('KNN-Optimum-actual-predicted-values.xlsx', index=False)

## KNN model for SubOptimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Bayesian Optimization Function (To obtain the lowest R value)
def knn_evaluate(n_neighbors, weights, algorithm):
    model = KNeighborsRegressor(
        n_neighbors=int(n_neighbors),
        weights='uniform' if weights < 0.5 else 'distance',  # 0 = uniform, 1 = distance
        algorithm='ball_tree' if algorithm < 0.5 else ('kd_tree' if algorithm < 1.5 else 'brute'),
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return -corr_coef

# Parameter ranges
pbounds = {
    'n_neighbors': (1, 50),  # K
    'weights': (0, 1),  # 'uniform' (0) or 'distance' (1)
    'algorithm': (0, 2)  # 'ball_tree' (0), 'kd_tree' (1), 'brute' (2)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=knn_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the lowest correlation value
best_params = optimizer.max['params']
best_n_neighbors = int(best_params['n_neighbors'])
best_weights = 'uniform' if best_params['weights'] < 0.5 else 'distance'
best_algorithm = 'ball_tree' if best_params['algorithm'] < 0.5 else ('kd_tree' if best_params['algorithm'] < 1.5 else 'brute')

# Create model for suboptimum condition
worst_model = KNeighborsRegressor(
    n_neighbors=best_n_neighbors,
    weights=best_weights,
    algorithm=best_algorithm
)

# Model training
worst_model.fit(X_train, y_train)
y_pred = worst_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition: n_neighbors={best_n_neighbors}, weights={best_weights}, algorithm={best_algorithm}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")
print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred
})

results_df.to_excel('KNN-SubOptimum-actual-predicted-values.xlsx', index=False)

## DT model for Optimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from sklearn.tree import DecisionTreeRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the highest R value)
def dt_evaluate(max_depth, min_samples_split, min_samples_leaf):
    model = DecisionTreeRegressor(
        max_depth=int(max_depth),
        min_samples_split=int(min_samples_split),
        min_samples_leaf=int(min_samples_leaf)
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return corr_coef

# Parameter ranges
pbounds = {
    'max_depth': (1, 20),
    'min_samples_split': (2, 20),
    'min_samples_leaf': (1, 20)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=dt_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the highest correlation value
best_params = optimizer.max['params']
best_max_depth = int(best_params['max_depth'])
best_min_samples_split = int(best_params['min_samples_split'])
best_min_samples_leaf = int(best_params['min_samples_leaf'])

# Create model for optimum condition
best_model = DecisionTreeRegressor(
    max_depth=best_max_depth,
    min_samples_split=best_min_samples_split,
    min_samples_leaf=best_min_samples_leaf
)

# Model training
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for optimum condition:  max_depth={best_max_depth}, min_samples_split={best_min_samples_split},min_samples_leaf={best_min_samples_leaf}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")

print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test,
    'Predicred': y_pred
})
results_df.to_excel('DT-Optimum-actual-predicted-values.xlsx', index=False)

## DT model for SubOptimum

import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from bayes_opt import BayesianOptimization
from sklearn.tree import DecisionTreeRegressor

start_time = time.time()

data = pd.read_excel("Data1.xlsx")

X = data.iloc[:, :-1]  # inputs
y = data.iloc[:, -1]   # output

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalization (StandardScaler)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Bayesian Optimization Function (To obtain the lowest R value)
def dt_evaluate(min_samples_split, min_samples_leaf, max_depth):
    model = DecisionTreeRegressor(
        max_depth=int(max_depth),
        min_samples_split=int(min_samples_split),
        min_samples_leaf=int(min_samples_leaf)
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    corr_coef = np.corrcoef(y_test, y_pred)[0, 1]
    return -corr_coef

# Parameter ranges
pbounds = {
    'max_depth': (1, 20),
    'min_samples_split': (2, 20),
    'min_samples_leaf': (1, 20)
}

# Bayesian optimization
optimizer = BayesianOptimization(
    f=dt_evaluate,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# Obtain the parameters that give the lowest correlation value
best_params = optimizer.max['params']
best_max_depth = int(best_params['max_depth'])
best_min_samples_split = int(best_params['min_samples_split'])
best_min_samples_leaf = int(best_params['min_samples_leaf'])

# Create model for suboptimum condition
worst_model = DecisionTreeRegressor(
    max_depth=best_max_depth,
    min_samples_split=best_min_samples_split,
    min_samples_leaf=best_min_samples_leaf
)

worst_model.fit(X_train, y_train)
y_pred = worst_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
rae = (mae / np.mean(y_test)) * 100
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
end_time = time.time()
execution_time = end_time - start_time
n = len(y_test)

print(f"Parameters for Suboptimum condition: max_depth={best_max_depth}, min_samples_split={best_min_samples_split},min_samples_leaf={best_min_samples_leaf}")
print(f"Correlation coefficient: {np.corrcoef(y_test, y_pred)[0, 1]:.3f}")
print(f"Mean absolute error: {mae:.3f}")
print(f"Root mean squared error: {rmse:.3f}")
print(f"Relative absolute error: {rae:.3f} %")
print(f"Mean absolute percentage error (MAPE): {mape:.3f} %")
print(f"Total Number of Instances: {n}")
print(f"Execution Time: {execution_time:.2f} seconds")

results_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred
})

results_df.to_excel('DT-SubOptimum-actual-predicted-values.xlsx', index=False)

# T-test and cohen's
import numpy as np
from scipy.stats import ttest_rel

# R values of models
models = ["RF", "XGBoost", "LightGBM", "ElasticNet", "ADABoost", "GBR", "KNN", "DT"]
#r_max = np.array([0.945, 0.951, 0.931, 0.541, 0.943, 0.925, 0.910, 0.931])#CO-Optimum
#r_min = np.array([0.723, 0.889, 0.608, 0.324, 0.475, 0.717, 0.289, 0.484])#CO-SubOptimum
#r_max = np.array([0.916, 0.954, 0.941, 0.642, 0.930, 0.953, 0.934, 0.922])#CO2-Optimum
#r_min = np.array([0.651, 0.858, 0.717, 0.182, 0.543, 0.731, 0.175, 0.477])#CO2-SubOptimum
#r_max = np.array([0.965, 0.981, 0.972, 0.840, 0.975, 0.975, 0.979, 0.944])#H2-Optimum
#r_min = np.array([0.838, 0.941, 0.806, 0.241, 0.728, 0.931, 0.316, 0.714])#H2-SubOptimum
r_max = np.array([0.905, 0.933, 0.923, 0.462, 0.919, 0.924, 0.896, 0.747])#CH4-Optimum
r_min = np.array([0.654, 0.856, 0.574, 0.270, 0.404, 0.766, 0.427, 0.334])#CH4-SubOptimum

# t-Test
t_stat, p_value = ttest_rel(r_max, r_min)

# Cohen's d calculation
diff = r_max - r_min
mean_diff = np.mean(diff)
std_diff = np.std(diff, ddof=1)
cohens_d = mean_diff / std_diff

print("\nT-Test Results")
print(f"t-statistic: {t_stat:.2f}")
print(f"p-value: {p_value:.4f}")
print(f"Cohen's d: {cohens_d:.4f}")

# p-value
alpha = 0.05  # Significance level
if p_value < alpha:
    print("\nThere is a statistically significant difference between the groups.")
else:
    print("\nThere is no statistically significant difference between the groups.")
